---
title: "2103 Project"
output: pdf_document
date: "`r Sys.Date()`"
---

```{r setup}
library(knitr)
library(dplyr)
library(readxl)
library(ggplot2)
library(corrplot)
library(Hmisc)
library(caret)
library(leaps)
library(e1071)
library(ggcorrplot)
library(ranger)
library(caret)
library(data.table)
library(nnet)
library(NeuralNetTools)
library(smotefamily)
library(ROSE)
library(rpart)
library(ROCR)
library(pROC)
library(caTools)
```

```{r read data, echo=F}
data <- read.csv("card.csv", sep = ",", skip = 2, header = FALSE)
data_v <-data
header <- scan("card.csv",sep=",",nlines=2,what=character())
```


```{r data structure}
head(data)
glimpse(data)
str(data)
```

```{r data preprocessing checking for NA Values}

#Checking for NA values
any(is.na(data))

summary(data)
```

```{r Exploratory data analysis for Gender, Education and Marital status}
#Gender Table
table(data$V3)

#Education Table
table(data$V4)

#Marital Status 
table(data$V5)

```

```{r data pre-processing Gender, Education and Marital Status }
#Making a Gender column
data_v$GENDER = ifelse(data$V3 == 1, "Male", "Female")

# Firstly modify Education values, change values that are not 1,2,3 to 4.
data$V4 = ifelse(data$V4%in%c(0,4,5,6), 4, data$V4)

# Making an Education column (1 = graduate school; 2 = university; 3 = high school; 4 = others).
data_v$EDUCATION <- factor(data$V4,
                            labels = c("Graduate_school", "University", "High_school","Others"))

#Replacing Marriage 0 to 3 (1 = married; 2 = single; 3 = others)
data$V5 = ifelse(data$V5 == 0, 3,data$V5)

data_v$Marital_Status <- factor(data$V5,
                            labels = c("Married", "Single", "Others"))

#Changing data type of V3,V4,V5 to factors
data$V5 <- as.factor(data$V5)
data$V4 <- as.factor(data$V4)
data$V3 <- as.factor(data$V3)
```

``` {r correlation matrix}
#Correlation matrix
data_onlycat <- subset(data, select = c(c(V13,V14,V15,V17,V18,V19,V20,V21,V22,V23,V24,V25)))
corrplot(cor(data_onlycat))
```

```{r data visualisation of Age using histogram}
hist(data$V6, xlab="Age", main="Histogram of Age")
```

```{r data visualisation of Marital Status}

marital_status_plot <- ggplot(data_v, aes(x=Marital_Status))+
  geom_bar() + 
  labs(title="Marital Status") +
  stat_count(aes(label = ..count..), geom = "label")

marital_status_plot
```

```{r data visualisation of Credit limit using boxplot}
credit_balance_plot <-ggplot(data_v, aes(x=V2), xlab="Credit Limit") + geom_boxplot() + coord_flip() + labs(title="Boxplot Of Credit Limit",x="Credit Limit")+
  theme_classic()

credit_balance_plot

```



```{r data visualisation Gender plot}
data_v$default<- ifelse(data$V25 == 1, "default", "no default")

# Bar Graph for gender
gender_plot<- ggplot(data_v, aes(GENDER))+
  geom_bar(aes(fill=default), width = 0.5) + 
  labs(title="Gender") +
  stat_count(aes(label = ..count..), geom = "label")

gender_plot

```

```{r data visualisation Education plot}


# Bar graph for Education
education_plot <- data_v %>%
  count(EDUCATION, default) %>%
    group_by(EDUCATION) %>%
    mutate(n = n/sum(n) * 100) %>%
    ggplot() + 
    aes(factor(EDUCATION,
               levels = c("High_school", "University", "Graduate_school","Others")), n,
        fill = default, label = paste0(round(n, 2), "%")) + 
    geom_col() +
    geom_text(position=position_stack(0.5))+
    xlab("Education")+
    ylab("%")

education_plot


```

```{r feature selection for log reg}
chistat <- matrix(0, 10, 2)
col  <- ncol(data) - 1
pred <- as.factor(data[,25])
for (i in 1:10) {
  x <- as.factor(data[,2+i])
  tbl <- table(x, pred)
  chi2res <- chisq.test(tbl)
  print(chi2res)
  chistat[i, 1] <- chi2res$statistic
  chistat[i, 2] <- chi2res$p.value
}
chistat <- chistat[-4,]
df <- data.frame(chistat[,1:2])
names(df) <- c("chi2 stat", "p-value")
df
```

```{r PartitioningData}
set.seed(1234)
n = length(data$V1)
index <- 1:nrow(data)
testindex <- sample(index, trunc(n)/4)
test.data <- data[testindex,]
train.data <- data[-testindex,]
```

```{r logistic regression2}
log_model <- glm(V25 ~., data = train.data, family = "binomial")
summary(log_model)
predict <- predict(log_model, test.data, type = "response")
predict <- ifelse(predict >0.2, 1, 0)
table <- table(test.data$V25, predict)
table
class_acc1 <- table[1,1]/(table[1,1] + table[1,2])
class_acc2 <- table[2,1]/(table[2,1] + table[2,2])
(class_acc1 + class_acc2)/2 #average class accuracy
1/((1/class_acc1)+(1/class_acc2)) #harmonic mean
mean(predict == test.data$V25) #accuracy
auc(test.data$V25, predict) #area under roc curve
table[2,2]/(table[2,1] + table[2,2]) #specificity
recall <- table[1,1]/(table[1,1] + table[1,2])
precision <- table[1,1]/(table[1,1] + table[2,1])
F2 <- 2*recall*precision/(recall + precision) #F2 statistic
```

```{r lm}
model <- lm(V25~., data = train.data)
summary(model)
#V1, V10-12, V14-18, V21-24
predict <- predict(model, test.data, type = "response")
predict <- ifelse(predict >0.5, 1, 0)
table(test.data$V25, predict)
mean(predict == test.data$V25)
better_model <- lm(V25~V2+V3+V4+V5+V6+V7+V8+V9+V13+V19+V20, data = train.data)
summary(better_model)
predict <- predict(better_model, test.data, type = "response")
predict <- ifelse(predict >0.5, 1, 0)
table(test.data$V25, predict)
mean(predict == test.data$V25)
predict <- predict(better_model, train.data, type = "response")
predict <- ifelse(predict >0.5, 1, 0)
table(train.data$V25, predict)
mean(predict == train.data$V25)
```
```{r feature selection for svm}
outforward <- regsubsets(data$V25 ~., data = data, method = "forward")
summary(outforward)
plot(outforward, scale="r2")

# inbackward <- regsubsets(data$V25 ~., data = data, method="backward")
# summary(inbackward)
# plot(inbackward, scale="r2")

#V7, V13, V8, V6, V9, V19, V5, V2
```

```{r SVM}
svm.model.feature.selection <- svm(as.factor(V25) ~ V7 + V13 + V8,
                                   data = train.data, type="C-classification",
                                   kernel="linear", cost=0.2, 
                                   class.weights=c("0"=0.185, "1"=0.815))
result_train_feature_selection <- predict(svm.model.feature.selection, train.data[,-25])
result_test_feature_selection <- predict(svm.model.feature.selection, test.data[,-25])
table(pred=result_train_feature_selection, actual=train.data$V25)
table(pred=result_test_feature_selection, actual=test.data$V25)
mean(result_train_feature_selection == train.data$V25)
mean(result_test_feature_selection == test.data$V25)

# With cost=0.2, (0.2,0.8) (test)
#     actual
# pred    0    1
#    0 5065  758
#    1  835  842
# 0.7876

```

``` {r NeuralNetwork}
#BEFORE BALANCING
#TRAIN
nn <- nnet(train.data$V25 ~., train.data[,1:24],maxit=1000,size=20,entropy=TRUE, decay = 0.01)
pred <- predict(nn, data = train.data[,1:24])
train.binpred <- predict(nn, train.data[,1:24], type = c("class"))
mean(train.data$V25 == train.binpred)
auc(train.data$V25, pred)

#TEST
predtest <- predict(nn, newdata = test.data[,1:24])
test.binpred <- predict(nn, test.data[,1:24], type = c("class"))
mean(test.data$V25 == test.binpred)
auc(test.data$V25, predtest)
```

``` {r decisiontree}
#BEFORE BALANCING

#TRAIN
tree.model <- rpart(V25 ~ ., data = train.data, method = "class", minbucket = 20)
tree.predict_train <- predict(tree.model, train.data, type = "class")
tree.binpredict_train <- predict(tree.model, train.data)
tree.ROCR_train <- prediction(tree.binpredict_train[,2], train.data$V25)
tree.auc_train <- as.numeric(performance(tree.ROCR_train,"auc")@y.values)
tree.auc_train
#TEST
tree.predict_test <- predict(tree.model, test.data, type = "class")
tree.binpredict_test <- predict(tree.model, test.data)
tree.ROCR_test <- prediction(tree.binpredict_test[,2], test.data$V25)
tree.auc_test <- as.numeric(performance(tree.ROCR_test,"auc")@y.values)
tree.auc_test
```

``` {r balancing}
#OVERSAMPLING
oversampled_train_data <- ovun.sample(V25 ~ ., data = train.data, method = "over",
                                      N = 2*nrow(subset(train.data, train.data$V25 == 0)))$data

table(oversampled_train_data$V25)
```

```{r svm AFTER BALANCING}

svm.model.feature.selection.balanced <- svm(as.factor(V25) ~ V7 + V13 + V8,
                                   data = oversampled_train_data, type="C-classification",
                                   kernel="linear", cost=0.2)
result_train_feature_selection_balanced <- predict(svm.model.feature.selection.balanced, oversampled_train_data[,-25])
result_test_feature_selection_balanced <- predict(svm.model.feature.selection.balanced, test.data[,-25])
table(pred=result_train_feature_selection_balanced, actual=oversampled_train_data$V25)
table(pred=result_test_feature_selection_balanced, actual=test.data$V25)
mean(result_train_feature_selection_balanced == oversampled_train_data$V25)
mean(result_test_feature_selection_balanced == test.data$V25)


```

``` {r logistic regression AFTER BALANCING}
log_model_balanced <- glm(V25 ~., data = oversampled_train_data, family = "binomial")
summary(log_model_balanced)
predictlr_balanced <- predict(log_model_balanced, test.data, type = "response")
predictlr_balanced <- ifelse(predictlr_balanced >0.5, 1, 0)
tablelr <- table(test.data$V25, predictlr_balanced)
tablelr
class_acc1 <- tablelr[1,1]/(tablelr[1,1] + tablelr[1,2])
class_acc2 <- tablelr[2,1]/(tablelr[2,1] + tablelr[2,2])
(class_acc1 + class_acc2)/2 #average class accuracy
1/((1/class_acc1)+(1/class_acc2)) #harmonic mean
mean(predictlr_balanced == test.data$V25) #accuracy
auc(test.data$V25, predictlr_balanced) #area under roc curve
tablelr[2,2]/(tablelr[2,1] + tablelr[2,2]) #specificity
recalllr <- tablelr[1,1]/(tablelr[1,1] + tablelr[1,2])
precisionlr <- tablelr[1,1]/(tablelr[1,1] + tablelr[2,1])
F2lr <- 2*recalllr*precisionlr/(recalllr + precisionlr) #F2 statistic
```

```{r lm after balance}

better_model_balanced <- lm(V25~V2+V3+V4+V5+V6+V7+V8+V9+V13+V19+V20, data = oversampled_train_data)
summary(better_model_balanced)
predict_balanced <- predict(better_model_balanced, test.data, type = "response")
predict_balanced <- ifelse(predict_balanced >0.5, 1, 0)
table(test.data$V25, predict_balanced)
mean(predict_balanced == test.data$V25)

predictlm <- predict(better_model_balanced, oversampled_train_data, type = "response")
predictlm <- ifelse(predictlm >0.5, 1, 0)
table(oversampled_train_data$V25, predictlm)
mean(predictlm == oversampled_train_data$V25)
```

```{r feature selection for svm}
outforward <- regsubsets(data$V25 ~., data = data, method = "forward")
summary(outforward)
plot(outforward, scale="r2")
```

```{r decisiontree AFTER BALANCING}
#AFTER BALANCING OVERSAMPLING
#TRAIN
tree.model_oversample <- rpart(V25 ~ ., data = oversampled_train_data, method = "class", minbucket = 20)
tree.predict_oversampletrain <- predict(tree.model_oversample, oversampled_train_data, type = "class")
tree.binpredict_oversampletrain <- predict(tree.model_oversample, oversampled_train_data)
tree.ROCR_oversampletrain <- prediction(tree.binpredict_oversampletrain[,2], oversampled_train_data$V25)
tree.auc_oversampletrain <- as.numeric(performance(tree.ROCR_oversampletrain,"auc")@y.values)
tree.auc_oversampletrain

#TEST
tree.predict_oversample <- predict(tree.model_oversample, test.data, type = "class")
tree.binpredict_oversample <- predict(tree.model_oversample, test.data)
tree.ROCR_oversample <- prediction(tree.binpredict_oversample[,2], test.data$V25)
tree.auc_oversample <- as.numeric(performance(tree.ROCR_oversample,"auc")@y.values)
tree.auc_oversample


```

``` {r NeuralNetworkbalanced}
#OVERSAMPLE
#TRAIN
nn_oversample <- nnet(oversampled_train_data$V25~., oversampled_train_data[,1:24],maxit=2000,size=20,entropy=TRUE, decay = 0.01)
predover <- predict(nn_oversample, data = oversampled_train_data)
train.binpredover <- predict(nn_oversample, oversampled_train_data, type = c("class"))
mean(oversampled_train_data$V25 == train.binpredover)
auc(oversampled_train_data$V25, predover)

#TEST
predovertest <- predict(nn_oversample, newdata = test.data)
testover.binpred <- predict(nn_oversample, test.data, type = c("class"))
mean(test.data$V25 == testover.binpred)
auc(test.data$V25, predovertest)


```
