---
title: "2103 Project"
output: pdf_document
date: "`r Sys.Date()`"
---

```{r setup}
library(knitr)
library(dplyr)
library(readxl)
library(ggplot2)
library(corrplot)
library(Hmisc)
library(caret)
library(leaps)
library(e1071)
library(ggcorrplot)
library(ranger)
library(caret)
library(data.table)
library(nnet)
library(NeuralNetTools)
library(smotefamily)
library(ROSE)
library("rpart")
library(ROCR)
library(pROC)
library("caTools")
```

```{r read data, echo=F}
data <- read.csv("card.csv", sep = ",", skip = 2, header = FALSE)
data_v <-data
header <- scan("card.csv",sep=",",nlines=2,what=character())
```


```{r data structure}
head(data)
glimpse(data)
str(data)
```

```{r data preprocessing checking for NA Values}

#Checking for NA values
any(is.na(data))

summary(data)
```
```{r Exploratory data analysis for Gender, Education and Marital status}
#Gender Table
table(data$V3)

#Education Table
table(data$V4)

#Marital Status 
table(data$V5)

```
```{r data pre-processing Gender, Education and Marital Status }
#Making a Gender column
data_v$GENDER = ifelse(data$V3 == 1, "Male", "Female")

# Firstly modify Education values, change values that are not 1,2,3 to 4.
data$V4 = ifelse(data$V4%in%c(0,4,5,6), 4, data$V4)

# Making an Education column (1 = graduate school; 2 = university; 3 = high school; 4 = others).
data_v$EDUCATION <- factor(data$V4,
                            labels = c("Graduate_school", "University", "High_school","Others"))

#Replacing Marriage 0 to 3 (1 = married; 2 = single; 3 = others)
data$V5 = ifelse(data$V5 == 0, 3,data$V5)

data_v$Marital_Status <- factor(data$V5,
                            labels = c("Married", "Single", "Others"))

```
``` {r correlation matrix}
#Correlation matrix
data_onlycat <- subset(data, select = c(c(V13,V14,V15,V17,V18,V19,V20,V21,V22,V23,V24,V25)))
corrplot(cor(data_onlycat))
```
```{r encoding values}
#Replacing values
# data$V7[data$V7 >2 ]<- 2
# data$V8[data$V8 >2 ]<- 2
# data$V9[data$V9 >2 ]<- 2
# data$V10[data$V10 >2 ]<- 2
# data$V11[data$V11 >2 ]<- 2
# data$V12[data$V12 >2 ]<- 2

```
```{r data visualisation of Age using histogram}
hist(data$V6, xlab="Age", main="Histogram of Age")
```
```{r data visualisation of Marital Status}

marital_status_plot <- ggplot(data_v, aes(x=Marital_Status))+
  geom_bar() + 
  labs(title="Marital Status") +
  stat_count(aes(label = ..count..), geom = "label")

marital_status_plot
```

```{r data visualisation of Credit limit using boxplot}
credit_balance_plot <-ggplot(data_v, aes(x=V2), xlab="Credit Limit") + geom_boxplot() + coord_flip() + labs(title="Boxplot Of Credit Limit",x="Credit Limit")+
  theme_classic()

credit_balance_plot

```



```{r data visualisation Gender plot}
data_v$default<- ifelse(data$V25 == 1, "default", "no default")

# Bar Graph for gender
gender_plot<- ggplot(data_v, aes(GENDER))+
  geom_bar(aes(fill=default), width = 0.5) + 
  labs(title="Gender") +
  stat_count(aes(label = ..count..), geom = "label")

gender_plot

```
```{r data visualisation Education plot}


# Bar graph for Education
education_plot <- data_v %>%
  count(EDUCATION, default) %>%
    group_by(EDUCATION) %>%
    mutate(n = n/sum(n) * 100) %>%
    ggplot() + 
    aes(factor(EDUCATION,
               levels = c("High_school", "University", "Graduate_school","Others")), n,
        fill = default, label = paste0(round(n, 2), "%")) + 
    geom_col() +
    geom_text(position=position_stack(0.5))+
    xlab("Education")+
    ylab("%")

education_plot


```

```{r feature selection}
outforward <- regsubsets(data$V25 ~., data = data, method = "forward")
summary(outforward)
plot(outforward, scale="r2")

# inbackward <- regsubsets(data$V25 ~., data = data, method="backward")
# summary(inbackward)
# plot(inbackward, scale="r2")

#V7, V13, V8, V6, V9, V19, V5, V2
```


```{r SVM}
n = length(data$V1)
index <- 1:nrow(data)
testindex <- sample(index, trunc(n)/4)
test.data <- data[testindex,]
train.data <- data[-testindex,]

svm.model.all.features <- svm(as.factor(V25) ~.,
                                   data = train.data, type="C-classification",
                                   kernel="linear")
result_train_all <- predict(svm.model.all.features, train.data[,-25])
result_test_all <- predict(svm.model.all.features, test.data[,-25])
table(pred=result_train_all, actual=train.data$V25)
table(pred=result_test_all, actual=test.data$V25)
mean(result_train_all == train.data$V25)
mean(result_test_all == test.data$V25)

# svm.model.all.features <- svm(as.factor(V25) ~.,
#                                    data = train.data, type="C-classification",
#                                    kernel="linear", cross=10)
# result_train_all <- predict(svm.model.all.features, train.data[,-25])
# result_test_all <- predict(svm.model.all.features, test.data[,-25])
# table(pred=result_train_all, actual=train.data$V25)
# table(pred=result_test_all, actual=test.data$V25)
# mean(result_train_all == train.data$V25)
# mean(result_test_all == test.data$V25)


# svm.model.feature.selection <- svm(as.factor(V25) ~ V7 + V13 + V8,
#                                    data = train.data, type="C-classification",
#                                    kernel="linear")
# result_train_feature_selection <- predict(svm.model.feature.selection, train.data[,-25])
# result_test_feature_selection <- predict(svm.model.feature.selection, test.data[,-25])
# table(pred=result_train_feature_selection, actual=train.data$V25)
# table(pred=result_test_feature_selection, actual=test.data$V25)
# mean(result_train_feature_selection == train.data$V25)
# mean(result_test_feature_selection == test.data$V25)

#      actual
# pred    0    1
#    0 5673 1243
#    1  181  403


svm.model.feature.selection <- svm(as.factor(V25) ~ V7 + V13 + V8,
                                   data = train.data, type="C-classification",
                                   kernel="linear", cost=0.2, 
                                   class.weights=c("0"=0.2, "1"=0.8))
result_train_feature_selection <- predict(svm.model.feature.selection, train.data[,-25])
result_test_feature_selection <- predict(svm.model.feature.selection, test.data[,-25])
table(pred=result_train_feature_selection, actual=train.data$V25)
table(pred=result_test_feature_selection, actual=test.data$V25)
mean(result_train_feature_selection == train.data$V25)
mean(result_test_feature_selection == test.data$V25)

# With cost=0.2, (0.2,0.8) (test)
#     actual
# pred    0    1
#    0 5065  758
#    1  835  842
# 0.7876

# With cost=0.2, class.weights=(0.3,0.7) (test)
#      actual
# pred    0    1
#    0 5335  905
#    1  502  758
# 0.8124

# With cost = 5, no class weights (test)
#      actual
# pred    0    1
#    0 5675 1255
#    1  175  395

```
``` {r NeuralNetwork}
#Hidden Layers = 20, decay = 0.01
#TRAIN
nn_hiddenlayer20 <- nnet(V25~., train.data,maxit=1000,size=20,entropy=TRUE, deacy = 0.01)
predhl20 <- predict(nn_hiddenlayer20, data = train.data)
train.binpredhl20 <- predict(nn_hiddenlayer20, train.data, type = c("class"))
mean(train.data$V25 == train.binpredhl20)
#0.779333
auc(train.data$V25, predhl20)
#0.6629

#TEST
predtest <- predict(nn_hiddenlayer20, data = test.data)
test.binpred <- predict(nn_hiddenlayer20, test.data, type = c("class"))
mean(test.data$V25 == test.binpred)
#0.7788
# !!!cant use auc here because length of test set not same not sure why

``` {r balancing}
#OVERSAMPLING
oversampled_train_data <- ovun.sample(V25 ~ ., data = train.data, method = "over",
                                      N = 2*nrow(subset(train.data, train.data$V25 == 0)))$data

table(oversampled_train_data$V25)
#UNDERSAMPLING
undersampled_train_data <- ovun.sample(V25 ~ ., data = train.data, method = "under",
                                      N = 2*nrow(subset(train.data, train.data$V25 == 1)))$data
table(undersampled_train_data$V25)
```
``` {r NeuralNetworkbalanced}
#OVERSAMPLE
#TRAIN
nn_oversample <- nnet(V25~., oversampled_train_data,maxit=2000,size=20,entropy=TRUE, deacy = 0.01)
predover <- predict(nn_oversample, data = oversampled_train_data)
train.binpredover <- predict(nn_oversample, oversampled_train_data, type = c("class"))
mean(oversampled_train_data$V25 == train.binpredover)
#0.5981
auc(oversampled_train_data$V25, predover)
#0.6351

#TEST
predovertest <- predict(nn_oversample, data = test.data)
testover.binpred <- predict(nn_oversample, test.data, type = c("class"))
mean(test.data$V25 == testover.binpred)
#0.6134667

#UNDERSAMPLE
#TRAIN
nn_undersample <- nnet(V25~., undersampled_train_data,maxit=2000,size=30,entropy=TRUE, deacy = 0.005)
predunder <- predict(nn_undersample, data = undersampled_train_data)
train.binpredunder <- predict(nn_undersample, undersampled_train_data, type = c("class"))
mean(undersampled_train_data$V25 == train.binpredunder)
#0.6080918
auc(undersampled_train_data$V25, predunder)
#0.6642

#TEST
predundertest <- predict(nn_undersample, data = test.data)
testunder.binpred <- predict(nn_undersample, test.data, type = c("class"))
mean(test.data$V25 == testunder.binpred)

```
