---
title: "BT2103 Project"
header-includes:
  - \usepackage{pdflscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
output: pdf_document
date: "17 November 2022"
---
```{r setup, echo = F, include = F}
library(knitr)
library(dplyr)
library(readxl)
library(ggplot2)
library(corrplot)
library(Hmisc)
library(caret)
library(leaps)
library(e1071)
library(ggcorrplot)
library(ranger)
library(caret)
library(data.table)
library(nnet)
library(NeuralNetTools)
library(smotefamily)
library(ROSE)
library(rpart)
library(ROCR)
library(pROC)
library(caTools)
library(partykit)
library(grid)
library(mvtnorm)
library(libcoin)
library(mlbench)
library(randomForest)
```

```{r img-with-knitr, echo=F, fig.align='center', out.width='70%'}
knitr::include_graphics("nus_logo_full-vertical.jpg")
```

```{r name, echo=FALSE, fig.align='center', out.width='100%'}
nameV <- c("Cheong Wen Wei", "Gao Heng", "Hom Lim Jun How", "Lee Jun Wei")
numV <- c("A0233582E", "A0234014X", "A0235131W", "A0230329M")
cover <- data.frame(nameV, numV)
colnames(cover) <- c("Student Name", "Student Number")
kable(cover, align='c', padding=30)
```
\pagenumbering{gobble}
\pagebreak
\pagenumbering{arabic}
\setcounter{page}{2}
\setcounter{tocdepth}{4}
\tableofcontents
\newpage
```{r read data, echo=F}
data <- read.csv("card.csv", sep = ",", skip = 2, header = FALSE)
data_v <-data
header <- scan("card.csv",sep=",",nlines=2,what=character())
```
# Overview
## Problem Description
<p align="justify">
This project endeavours to accurately predict customers who will default on their bills from those who will pay promptly. More importantly, being able to accurately predict customers who will default would allow banks to minimised potential losses that would potentially be written off as bad debt. Therefore, a stronger emphasis is being placed on being able to accurately predict customers who will default.
</p>
## Data
<p align="justify">
The dataset used for this project contains information of 30,000 credit card holders obtained from a bank in Taiwan. Each credit card holder is described by 23 feature attributes, a unique customer identification corresponding to each credit card holder as well as each credit card holder's default status.
</p>
# Exploratory Data Analysis
## Structure of the Data
<p align="justify">
The first crucial step is to find out more about the dataset. By exploring the structure of the data, it can be discerned that all variables read in were of type integer. However, it is clear that variables V3 (Gender), V4 (Education Level), V5 (Marital Status) and V7 to V12 (Repayment Status) should not be treated as integers. Instead, the aforementioned variables would be converted to a factor to better represent the data.
</p>
```{r data structure, echo=F}
str(data)
```
\newpage
## Missing Values
<p align="justify">
In this section, a check was performed to identify any missing or N.A. values. From the check, it was identified that there were no missing or N.A. values in the data. Thereafter, a summary of the variables is shown with the exception of V1 which is the customer identification.
</p>
```{r data preprocessing checking for NA Values, echo=F}
#Checking for NA values
#any(is.na(data))

summary(data[2:25])
```
## Distribution of the Data
### Target Variable
<p align="justify">
The distribution of the target variable was explored using the table function in R. 
</p>
```{r Exploratory Data Analysis for V25}
# Default Table
table(data$V25)
```
<p align="justify">
It can be observed that the dataset is imbalance with approximately 78% of the 30,000 observations being not default while the remaining 22% make up the default customers. 
</p>
### Caterogical Variables
<p align="justify">
The distribution of the categorical variables were explored using the table function in R. 
</p>
```{r Exploratory data analysis for Gender, Education and Marital status}
#Gender Table
table(data$V3)

#Education Table
table(data$V4)

#Marital Status 
table(data$V5)
```
<p align="justify">
It was observed that Education has unknown observations (values of 5 and 6) and Marriage has unknown observations (value of 0). These inconsistencies will be addressed subsequently under the Data Pre-Processing Section. 
</p> 
\pagebreak
### Continuous Variables
<p align="justify">
The correlation matrix was used to check the degree of association among the continuous variables from the dataset. From the visualisation, it is evident that V13, V14, V15, V16, V17 and V18 are highly correlated. This is probably due to autocorrelation where the bill amount from the month before affects the bill amount in the current month. As such, feature engineering would be used to overcome the autocorrelation which would be elaborated under the Data Pre-Processing section.     
</p>
``` {r correlation matrix, fig.align = "center", echo = F}
#Correlation matrix
data_onlycat <- subset(data, select = c(c(V2,V13,V14,V15,V16,V17,V18,V19,V20,V21,V22,V23,V24)))
ggcorrplot(cor(data_onlycat), method = "circle", lab = T, lab_size = 2)
```
\pagebreak
# Data Pre-Processing and Feature Engineering
<p align="justify"> 
As highlighted in the Exploratory Data Analysis section, there were inconsistencies with the data as well as the problem of autocorrelation. In order to address the inconsistencies in values for the Education, observations that had 0, 4, 5 or 6 as the value for Education would be categorised under 4 as "Others". Similarly, observations that had 0 under the Marriage feature would be categorised under the value 3 as "Others". 
</p>
<p align="justify">
In order to resolve the possible autocorrelation among the features, V13 to V18 as well as V19 to V24, 2 new features would be introduced to represent V13 to V18 and V19 to V24. The first new feature is `mean_col_13_18` which is the average of V13 to V18. Similarly, the second new feature is `mean_col_19_24` would be the average of V19 to V24.
</p>
```{r data pre-processing, echo = T}
#Making a Gender column
data_v$Gender = ifelse(data$V3 == 1, "Male", "Female")

# Firstly modify Education values, change values that are not 1,2,3 to 4.
data$V4 = ifelse(data$V4%in%c(0,4,5,6), 4, data$V4)

# Making an Education column (1 = graduate school; 
#       2 = university; 3 = high school; 4 = others).
data_v$Education <- factor(data$V4,
          labels = c("Graduate School", "University", "High School","Others"))

#Replacing Marriage 0 to 3 (1 = married; 2 = single; 3 = others)
data$V5 = ifelse(data$V5 == 0, 3,data$V5)

data_v$`Marital Status` <- factor(data$V5,
                            labels = c("Married", "Single", "Others"))

#Changing data type to factors
data$V5 <- as.factor(data$V5)
data$V4 <- as.factor(data$V4)
data$V3 <- as.factor(data$V3)

data$V7 <- as.factor(data$V7)
data$V8 <- as.factor(data$V8)
data$V9 <- as.factor(data$V9)
data$V10 <- as.factor(data$V10)
data$V11 <- as.factor(data$V11)
data$V12 <- as.factor(data$V12)

#Feature engineering: compress 6 columns to 1 by finding the average of each observation
data_MOD <- mutate(data, mean_col_13_18 = rowMeans(select(data,V13:V18), na.rm = TRUE))
data_MOD <- mutate(data_MOD, mean_col_19_24 = rowMeans(select(data,V19:V24),
                                                       na.rm = TRUE))
```
<p align="justify">
After the pre-processing of the data, plots were created to visualised the "cleaned" data.
</p>
\pagebreak
## Feature: Age
<p align="justify">
The age distribution across the 30,000 credit card holders is shown below. 
</p>
```{r data visualisation of Age using histogram, fig.height=4,fig.align = "center", echo = F}
hist(data$V6, xlab="Age", main="Histogram of Age", las=1, col="skyblue", cex.axis=0.8)
```
<p align="justify">
From the above histogram, it appears to be positively skewed with most of the credit card holders being younger than 50 years old. 
</p>
\pagebreak
## Feature: Marital Status
<p align="justify">
The marital status distribution across the 30,000 credit card holders is shown below. 
</p>
```{r data visualisation of Marital Status, fig.align = "center", fig.height=4,echo = F, warning = F}

marital_status_plot <- ggplot(data_v, aes(x=`Marital Status`))+
  geom_bar() + 
  labs(title="Marital Status") +
  stat_count(aes(label = ..count..), geom = "label")

marital_status_plot
```
<p align="justify">
From the above bar plot, only a few customers have marital status of "Others" while the majority are either "Married" or "Single" with a slightly higher frequency of "Single" customers.
</p>
\pagebreak
## Feature: Credit Limit Balance
<p align="justify">
The credit limit balance distribution across the 30,000 credit card holders is shown below. 
</p>
```{r data visualisation of Credit limit using boxplot,fig.height=4, fig.align = "center", echo = F}
boxplot(data$V2/1000, main="Boxplot for Credit Limit of all Customers", ylab="Credit Limit (NT dollar) (in thousands)",
        las=1, cex.axis=0.8)

```
<p align="justify">
From the above box plot, the median credit limit of the 30,000 customers is approximately TWD160,000 with a few outliers that have a credit limit of approximately TWD550,000 or higher. 
</p>
\pagebreak
## Feature: Gender
<p align="justify">
The gender distribution across the 30,000 credit card holders is shown below. 
</p>
```{r data visualisation Gender plot, fig.align = "center",fig.height=4, echo = F}
data_v$Default<- ifelse(data$V25 == 1, "Default", "Did not default")

# Bar Graph for gender
gender_plot<- ggplot(data_v, aes(Gender))+
  geom_bar(aes(fill=Default), width = 0.5) + 
  labs(title="Gender") +
  stat_count(aes(label = ..count..), geom = "label")

gender_plot
```
<p align="justify">
From the above stacked bar plot, there is slightly more female than male customers with similar proportion of defaults within each gender group. 
</p>
\pagebreak
## Feature: Education
<p align="justify">
The education distribution across the 30,000 credit card holders is shown below. 
</p>
```{r data visualisation Education plot, fig.height=4, fig.align = "center",echo = F}
# Bar graph for Education
education_plot <- data_v %>%
  count(Education, Default) %>%
    group_by(Education) %>%
    mutate(n = n/sum(n) * 100) %>%
    ggplot() + 
    aes(factor(Education,
               levels = c("High School", "University", "Graduate School","Others")), n,
        fill = Default, label = paste0(round(n, 2), "%")) + 
    geom_col() +
    geom_text(position=position_stack(0.5))+
    labs(title="Education")+
    xlab("Education")+
    ylab("Proportion (%)")

education_plot
```
<p align="justify">
The above stacked bar plot shows the default proportion within the individual education level group. From the plot, it appears that customers who have "Others" as their education level has the least proportion of default.
</p>
\pagebreak
# Partioning Data
<p align="justify">
Prior to the training of the models, the dataset would now be split into 75% training data and 25% testing data. 
</p>
```{r PartitioningData, echo = T}
set.seed(1234)
n = length(data$V1)
index <- 1:nrow(data)
testindex <- sample(index, trunc(n)/4)
test.data <- data_MOD[testindex,]
train.data <- data_MOD[-testindex,]
```
# Feature Selection and Model Selection
<p align="justify">
In order to prevent overfitting of the models, it is prudent to find the optimal number of features to build the models such that it is robust and has the ability to generalise. As such, one method to find the optimal number of features to use would be to construct the Variable Importance Plot. This was accomplished by first creating a random forest using 10-fold cross-validation and plotting the variable importance of the random forest.
</p>
```{r FeatureSelection, echo=F}
# control <- trainControl(method="cv", number=10)
# model <- train(train.data[,c(2:12, 26:27)], as.factor(train.data$V25), 
#                method="rf", trControl=control)
# plot(varImp(model, scale=F), main="Variable Importance Plot")
```

```{r FeatureSelectionImage, echo=F, fig.align='center', out.width='80%'}
knitr::include_graphics("Variable Importance Plot.png")
```
<p align="justify">
From the Variable Importance Plot, it can be seen that V7 (Repayment Status in September 2005) has the highest importance value, indicating that V7 should definitely be included in the models. In order to ensure that the models are representative and would not overfit, the features chosen to be included in the models, based on its individual importance, are `V7`, `mean_col_19_24` (the average payment amount), `mean_col_13_18` (the average bill amount payable), `V6` (Age), `V2` (credit limit) and `V8` (Repayment Status in August 2005).
</p>
\pagebreak
## Logistic Regression Model
<p align="justify">
The first model built to predict whether a customer would default on his or her payments is the logistic regression model. A logistic regression model is appropriate because the target variable is discrete (either default or not default). One benefit of building a logistic regression model is that it is easy to build and train the model. However, a drawback of logistic regression is the assumption that the target variable has a linear relationship with the independent variables.  
</p>
```{r logistic regression, echo = F}
log_model <- glm(as.factor(V25)~ V7 + mean_col_19_24 + mean_col_13_18 + V6 + V2 + V8, 
                 data = train.data, family = "binomial")
#summary(log_model)
predict <- predict(log_model, test.data, type = "response")
predict <- ifelse(predict > 0.21, 1, 0)
table <- table(pred=predict, actual=test.data$V25)
table #confusion matrix
#class_acc1 <- table[1,1]/(table[1,1] + table[1,2])
#class_acc2 <- table[2,1]/(table[2,1] + table[2,2])
#(class_acc1 + class_acc2)/2 #average class accuracy
#1/((1/class_acc1)+(1/class_acc2)) #harmonic mean
acc <- mean(predict == test.data$V25) #accuracy
roc <- auc(test.data$V25, predict) #area under roc curve
specificity <- table[2,2]/(table[2,1] + table[2,2]) #specificity
recall <- table[1,1]/(table[1,1] + table[1,2])
precision <- table[1,1]/(table[1,1] + table[2,1])
F1 <- 2*recall*precision/(recall + precision) #F1 statistic
log_metrics <- c(acc, specificity, roc, F1)
# metrics <- rbind(metrics, log_metrics)
metrics <- t(as.data.frame(log_metrics))
```
<p align="justify">
Based on the confusion matrix generated by the prediction of the logistic regression model, there are a total of 5,769 correctly classified defaults and non-defaults. More importantly, there are 754 defaulters that were incorrectly classified as non-defaulters.  
</p>
<p align="justify">
After running the model, below are the results of the logistic regression model. 
</p>
```{r logisticResults, echo=F}
log_metric <- log_metrics
log_df <- data.frame(t(log_metric))
colnames(log_df) <- c("Accuracy", "Specificity", "Area under ROC Curve", "F1-Score")
kable(round(log_df, 2), align='c')
```

\pagebreak
## Support Vector Machine
<p align="justify">
The second model built to predict whether a customer would default on his or her payment is the Support Vector Machine (SVM). Based on the features chosen, the support vector machine was trained using a linear kernel, a cost of 10 and class weights of 0.17 for non-default and 0.83 for default. 
</p>
```{r SVM, echo = F}
svm.model.feature.selection <- svm(V25 ~ V7 + mean_col_19_24 + mean_col_13_18 + V6 + V2 + V8,
                                    data = train.data, type="C-classification",
                                    kernel="linear", cost=10, 
                                    class.weights=c("0"=0.17, "1"=0.83))


#result_train_feature_selection <- predict(svm.model.feature.selection, train.data[,-25])
result_test_feature_selection <- predict(svm.model.feature.selection, test.data)
#table(pred=result_train_feature_selection, actual=train.data$V25)
tablesvm <- table(pred=result_test_feature_selection, actual=test.data$V25) #confusion matrix
tablesvm
#mean(result_train_feature_selection == train.data$V25)
#mean(result_test_feature_selection == test.data$V25)

#class_accsvm1 <- tablesvm[1,1]/(tablesvm[1,1] + tablesvm[1,2])
#class_accsvm2 <- tablesvm[2,1]/(tablesvm[2,1] + tablesvm[2,2])
#(class_accsvm1 + class_accsvm2)/2 #average class accuracy
#1/((1/class_accsvm1)+(1/class_accsvm2)) #harmonic mean
accsvm <- mean(result_test_feature_selection == test.data$V25) #accuracy
rocsvm <- auc(test.data$V25, as.numeric(result_test_feature_selection)) #area under roc curve
specificitysvm <- tablesvm[2,2]/(tablesvm[2,1] + tablesvm[2,2]) #specificity
recallsvm <- tablesvm[1,1]/(tablesvm[1,1] + tablesvm[1,2])
precisionsvm <- tablesvm[1,1]/(tablesvm[1,1] + tablesvm[2,1])
F1svm <- 2*recallsvm*precisionsvm/(recallsvm + precisionsvm) #F1 statistic
svm_metrics <- c(accsvm, specificitysvm, rocsvm, F1svm)
metrics <- rbind(metrics, svm_metrics)
#V2,3,4,5,6,7,8,9,13,19,20,22
# With cost=10, (0.17,0.83) (test)
#     actual
# pred    0    1
#    0 4964  813
#    1  868  855
# 0.7758
```
<p align="justify">
Based on the confusion matrix generated by the prediction of the support vector machine, there are a total of 5,819 correctly classified defaults and non-defaults. More importantly, there are 813 defaulters that were incorrectly classified as non-defaulters.  
</p>
<p align="justify">
After running the model, below are the results of the support vector machine. 
</p>
```{r svmResults, echo=F}
svm_metric <- svm_metrics
svm_df <- data.frame(t(svm_metric))
colnames(svm_df) <- c("Accuracy", "Specificity", "Area under ROC Curve", "F1-Score")
kable(round(svm_df, 2), align='c')
```
\pagebreak
## Neural Network
<p align="justify">
The third model built to predict whether a customer would default on his or her payment is the Neural Network. Using the features chosen, the neural network has 6 input neurons, 15 hidden neurons in the hidden layer and 2 output neurons. Additional parameters include a max iteration of 1,000 a decay of 0.01 as well as using entropy (maximum conditional likelihood).
</p>
``` {r NeuralNetwork, echo = F, include=F}
set.seed(1234)
nn <- nnet(train.data$V25 ~ V7 + V2 + V6 + `mean_col_13_18` + `mean_col_19_24` + V8, train.data,
            maxit=1000,size=15,entropy=TRUE, decay = 0.01)

#TEST
#predtest <- predict(nn, data = test.data)
test.binpred <- predict(nn, newdata=test.data, type = c("class"))
#mean(test.data$V25 == test.binpred)
#auc(test.data$V25, predtest)
tablenn <- table(pred=test.binpred, actual=test.data$V25)
tablenn #confusion matrix
#class_accnn1 <- tablenn[1,1]/(tablenn[1,1] + tablenn[1,2])
#class_accnn2 <- tablenn[2,1]/(tablenn[2,1] + tablenn[2,2])
#(class_accnn1 + class_accnn2)/2 #average class accuracy
#1/((1/class_accnn1)+(1/class_accnn2)) #harmonic mean
accnn <- mean(test.binpred == test.data$V25) #accuracy
rocnn <- auc(test.data$V25, as.numeric(test.binpred)) #area under roc curve
specificitynn <- tablenn[2,2]/(tablenn[2,1] + tablenn[2,2]) #specificity
recallnn <- tablenn[1,1]/(tablenn[1,1] + tablenn[1,2])
precisionnn <- tablenn[1,1]/(tablenn[1,1] + tablenn[2,1])
F1nn <- 2*recallnn*precisionnn/(recallnn + precisionnn) #F1 statistic
nn_metrics <- c(accnn, specificitynn, rocnn, F1nn)
metrics <- rbind(metrics, nn_metrics)
```

```{r  nnConfusionMatrix}
tablenn
```

<p align="justify">
Based on the confusion matrix generated by the prediction of the neural network, there are a total of 6,093 correctly classified defaults and non-defaults. More importantly, there are 1,139 defaulters that were incorrectly classified as non-defaulters.  
</p>
<p align="justify">
After running the model, below are the results of the neural network. 
</p>
```{r nnResults, echo=F}
nn_metric <- nn_metrics
nn_df <- data.frame(t(nn_metric))
colnames(nn_df) <- c("Accuracy", "Specificity", "Area under ROC Curve", "F1-Score")
kable(round(nn_df, 2), align='c')
```
\pagebreak
\blandscape

\def\fillandplacepagenumber{%
 \par\pagestyle{empty}%
 \vbox to 0pt{\vss}\vfill
 \vbox to 10pt{\baselineskip0pt
   \hbox to\linewidth{\hss}%
   \baselineskip\footskip
   \hbox to\linewidth{%
     \hfil\thepage\hfil}\vss}}

## Decision Tree
<p align="justify">
</p>
``` {r decisiontree, echo = F, fig.align='center', out.width='95%'}
#BEFORE BALANCING
tree.model<- ctree(as.factor(V25) ~ V7 + V13 + V8, train.data)
tree.predict_test <-predict(tree.model, test.data)
#plot(tree.model)

knitr::include_graphics("DT.jpg")

tabletree <- table(test.data$V25, tree.predict_test)
tabletree #confusion matrix
#class_acctree1 <- tabletree[1,1]/(tabletree[1,1] + tabletree[1,2])
#class_acctree2 <- tabletree[2,1]/(tabletree[2,1] + tabletree[2,2])
#(class_acctree1 + class_acctree2)/2 #average class accuracy
#1/((1/class_acctree1)+(1/class_acctree2)) #harmonic mean
acctree <- mean(tree.predict_test == test.data$V25) #accuracy
roctree <- auc(test.data$V25, as.numeric(tree.predict_test)) #area under roc curve
specificitytree <- tabletree[2,2]/(tabletree[2,1] + tabletree[2,2]) #specificity
recalltree <- tabletree[1,1]/(tabletree[1,1] + tabletree[1,2])
precisiontree <- tabletree[1,1]/(tabletree[1,1] + tabletree[2,1])
F1tree <- 2*recalltree*precisiontree/(recalltree + precisiontree) #F1 statistic
tree_metrics <- c(acctree, specificitytree, roctree, F1tree)
metrics <- rbind(metrics, tree_metrics)

row.names(metrics) <- c("Logistic Regression", "Support Vector Machine","Neural Network","Decision Tree")

colnames(metrics) <- c("Accuracy", "Specificity", "Area under ROC Curve", "F1-Score")
```
\fillandplacepagenumber

\elandscape
\pagebreak
# Model Evaluation
<p align="justify">
The metrics employed to evaluate the models are accuracy, specificity, area under ROC curve and F1-score.
</p>
```{r metrics, echo = F}

kable(round(metrics, 2), align='c')

```

```{r balancing code, echo = F}
# ``` {r balancing}
# #OVERSAMPLING
# oversampled_train_data <- ovun.sample(V25 ~ ., data = train.data, method = "over",
#                                       N = 2*nrow(subset(train.data, train.data$V25 == 0)))$data
# 
# table(oversampled_train_data$V25)
# ```
# 
# ```{r svm AFTER BALANCING}
# 
# svm.model.feature.selection.balanced <- svm(as.factor(V25) ~ V7 + V13 + V8,
#                                    data = oversampled_train_data, type="C-classification",
#                                    kernel="linear", cost=0.2)
# result_train_feature_selection_balanced <- predict(svm.model.feature.selection.balanced, oversampled_train_data[,-25])
# result_test_feature_selection_balanced <- predict(svm.model.feature.selection.balanced, test.data[,-25])
# table(pred=result_train_feature_selection_balanced, actual=oversampled_train_data$V25)
# table(pred=result_test_feature_selection_balanced, actual=test.data$V25)
# mean(result_train_feature_selection_balanced == oversampled_train_data$V25)
# mean(result_test_feature_selection_balanced == test.data$V25)
# 
# 
# ```
# 
# ``` {r logistic regression AFTER BALANCING}
# log_model_balanced <- glm(V25 ~., data = oversampled_train_data, family = "binomial")
# summary(log_model_balanced)
# predictlr_balanced <- predict(log_model_balanced, test.data, type = "response")
# predictlr_balanced <- ifelse(predictlr_balanced >0.5, 1, 0)
# tablelr <- table(test.data$V25, predictlr_balanced)
# tablelr
# class_acc1 <- tablelr[1,1]/(tablelr[1,1] + tablelr[1,2])
# class_acc2 <- tablelr[2,1]/(tablelr[2,1] + tablelr[2,2])
# (class_acc1 + class_acc2)/2 #average class accuracy
# 1/((1/class_acc1)+(1/class_acc2)) #harmonic mean
# mean(predictlr_balanced == test.data$V25) #accuracy
# auc(test.data$V25, predictlr_balanced) #area under roc curve
# tablelr[2,2]/(tablelr[2,1] + tablelr[2,2]) #specificity
# recalllr <- tablelr[1,1]/(tablelr[1,1] + tablelr[1,2])
# precisionlr <- tablelr[1,1]/(tablelr[1,1] + tablelr[2,1])
# F1lr <- 2*recalllr*precisionlr/(recalllr + precisionlr) #F1 statistic
# 
# ```
# 
# ```{r lm after balance}
# 
# better_model_balanced <- lm(V25~V2+V3+V4+V5+V6+V7+V8+V9+V13+V19+V20, data = oversampled_train_data)
# summary(better_model_balanced)
# predict_balanced <- predict(better_model_balanced, test.data, type = "response")
# predict_balanced <- ifelse(predict_balanced >0.5, 1, 0)
# table(test.data$V25, predict_balanced)
# mean(predict_balanced == test.data$V25)
# 
# predictlm <- predict(better_model_balanced, oversampled_train_data, type = "response")
# predictlm <- ifelse(predictlm >0.5, 1, 0)
# table(oversampled_train_data$V25, predictlm)
# mean(predictlm == oversampled_train_data$V25)
# ```
# 
# 
# ```{r decisiontree AFTER BALANCING}
# #AFTER BALANCING OVERSAMPLING
# #TRAIN
# tree.model_oversample<- ctree(as.factor(V25) ~ V7 + V13 + V8, oversampled_train_data)
# tree.binpredict_oversample <-predict(tree.model_oversample, test.data)
# 
# tabletreebal <- table(test.data$V25, tree.binpredict_oversample)
# class_acctreebal1 <- tabletreebal[1,1]/(tabletreebal[1,1] + tabletreebal[1,2])
# class_acctreebal2 <- tabletreebal[2,1]/(tabletreebal[2,1] + tabletreebal[2,2])
# (class_acctreebal1 + class_acctreebal2)/2 #average class accuracy
# 1/((1/class_acctreebal1)+(1/class_acctreebal2)) #harmonic mean
# mean(tree.binpredict_oversample == test.data$V25) #accuracy
# auc(test.data$V25, as.numeric(tree.binpredict_oversample)) #area under roc curve
# tabletreebal[2,2]/(tabletreebal[2,1] + tabletreebal[2,2]) #specificity
# recalltreebal <- tabletreebal[1,1]/(tabletreebal[1,1] + tabletreebal[1,2])
# recalltreebal
# precisiontreebal <- tabletreebal[1,1]/(tabletreebal[1,1] + tabletreebal[2,1])
# precisiontreebal
# F1treebal <- 2*recalltreebal*precisiontreebal/(recalltreebal + precisiontreebal) #F1 statistic
# F1treebal
# 
# plot(tree.model_oversample)
# 
# ```
# 
# ``` {r NeuralNetworkbalanced}
# #OVERSAMPLE
# #TRAIN
# nn_oversample <- nnet(oversampled_train_data$V25~., oversampled_train_data[,1:24],maxit=2000,size=20,entropy=TRUE, decay = 0.01)
# predover <- predict(nn_oversample, data = oversampled_train_data)
# train.binpredover <- predict(nn_oversample, oversampled_train_data, type = c("class"))
# mean(oversampled_train_data$V25 == train.binpredover)
# auc(oversampled_train_data$V25, predover)
# 
# #TEST
# predovertest <- predict(nn_oversample, newdata = test.data)
# testover.binpred <- predict(nn_oversample, test.data, type = c("class"))
# mean(test.data$V25 == testover.binpred)
# auc(test.data$V25, predovertest)
# 
# tablenn_balanced <- table(test.data$V25, testover.binpred)
# tablenn_balanced
# class_accnnbal1 <- tablenn_balanced[1,1]/(tablenn_balanced[1,1] + tablenn_balanced[1,2])
# class_accnnbal2 <- tablenn_balanced[2,1]/(tablenn_balanced[2,1] + tablenn_balanced[2,2])
# (class_accnnbal1 + class_accnnbal2)/2 #average class accuracy
# 1/((1/class_accnnbal1)+(1/class_accnnbal2)) #harmonic mean
# mean(testover.binpred == test.data$V25) #accuracy
# auc(test.data$V25, predovertest) #area under roc curve
# tablenn_balanced[2,2]/(tablenn_balanced[2,1] + tablenn_balanced[2,2]) #specificity
# recallnn_balanced <- tablenn_balanced[1,1]/(tablenn_balanced[1,1] + tablenn_balanced[1,2])
# recallnn_balanced
# precisionnnbal <- tablenn_balanced[1,1]/(tablenn_balanced[1,1] + tablenn_balanced[2,1])
# precisionnnbal
# F1nnbal <- 2*recallnn_balanced*precisionnnbal/(recallnn_balanced + precisionnnbal) #F1 statistic
# F1nnbal
# 
# ```
```
